{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loaders import *\n",
    "from energy_helpers import get_energy\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HamidaEtAl(nn.Module):\n",
    "    \"\"\"\n",
    "    3-D Deep Learning Approach for Remote Sensing Image Classification\n",
    "    Amina Ben Hamida, Alexandre Benoit, Patrick Lambert, Chokri Ben Amar\n",
    "    IEEE TGRS, 2018\n",
    "    https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344565\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv3d):\n",
    "            init.kaiming_normal_(m.weight)\n",
    "            init.zeros_(m.bias)\n",
    "\n",
    "    def __init__(self, input_channels, n_classes, patch_size=5, dilation=1):\n",
    "        super(HamidaEtAl, self).__init__()\n",
    "        # The first layer is a (3,3,3) kernel sized Conv characterized\n",
    "        # by a stride equal to 1 and number of neurons equal to 20\n",
    "        self.patch_size = patch_size\n",
    "        self.input_channels = input_channels\n",
    "        dilation = (dilation, 1, 1)\n",
    "\n",
    "        if patch_size == 3:\n",
    "            self.conv1 = nn.Conv3d(\n",
    "                1, 22, (3, 3, 3), stride=(1, 1, 1), dilation=dilation, padding=1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv3d(\n",
    "                1, 20, (3, 3, 3), stride=(1, 1, 1), dilation=dilation, padding=0)\n",
    "        # Next pooling is applied using a layer identical to the previous one\n",
    "        # with the difference of a 1D kernel size (1,1,3) and a larger stride\n",
    "        # equal to 2 in order to reduce the spectral dimension\n",
    "        self.pool1 = nn.Conv3d(\n",
    "            20, 20, (3, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "        # Then, a duplicate of the first and second layers is created with\n",
    "        # 35 hidden neurons per layer.\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            20, 35, (3, 3, 3), dilation=dilation, stride=(1, 1, 1), padding=(1, 0, 0))\n",
    "        self.pool2 = nn.Conv3d(\n",
    "            35, 35, (3, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "        # Finally, the 1D spatial dimension is progressively reduced\n",
    "        # thanks to the use of two Conv layers, 35 neurons each,\n",
    "        # with respective kernel sizes of (1,1,3) and (1,1,2) and strides\n",
    "        # respectively equal to (1,1,1) and (1,1,2)\n",
    "        self.conv3 = nn.Conv3d(\n",
    "            35, 35, (3, 1, 1), dilation=dilation, stride=(1, 1, 1), padding=(1, 0, 0))\n",
    "        self.conv4 = nn.Conv3d(\n",
    "            35, 35, (2, 1, 1), dilation=dilation, stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "\n",
    "        #self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.features_size = self._get_final_flattened_size()\n",
    "        # The architecture ends with a fully connected layer where the number\n",
    "        # of neurons is equal to the number of input classes.\n",
    "        self.fc = nn.Linear(self.features_size, n_classes)\n",
    "\n",
    "        # self.apply(self.weight_init)\n",
    "\n",
    "    def _get_final_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros((1, 1, self.input_channels,\n",
    "                             self.patch_size, self.patch_size))\n",
    "            x = self.pool1(self.conv1(x))\n",
    "            x = self.pool2(self.conv2(x))\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv4(x)\n",
    "            _, t, c, w, h = x.size()\n",
    "        return t * c * w * h\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(-1, self.features_size)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv3d(1, 20, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
      "Conv3d(20, 20, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0))\n",
      "Conv3d(20, 35, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "Conv3d(35, 35, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0))\n",
      "Conv3d(35, 35, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0))\n",
      "Conv3d(35, 35, kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0))\n",
      "[torch.Size([8, 1, 220, 5, 5]), torch.Size([8, 20, 218, 3, 3]), torch.Size([8, 20, 109, 3, 3]), torch.Size([8, 35, 109, 1, 1]), torch.Size([8, 35, 55, 1, 1]), torch.Size([8, 35, 55, 1, 1]), torch.Size([8, 35, 28, 1, 1]), torch.Size([8, 16])]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'designs/system_manual/arch/system_arch_1x16.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m HamidaEtAl(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m220\u001b[39m, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m220\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m total, layerwise \u001b[38;5;241m=\u001b[39m \u001b[43mget_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal energy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayerwise energy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, layerwise)\n",
      "File \u001b[0;32m~/lab3/energy_helpers.py:66\u001b[0m, in \u001b[0;36mget_energy\u001b[0;34m(model, x, hw_arch_path, hw_components_dir_path, mapper_config_path, layers_path, verbose)\u001b[0m\n\u001b[1;32m     63\u001b[0m map_path \u001b[38;5;241m=\u001b[39m PosixPath(map_path)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# stats, _ = run_timeloop_model(hw_arch_path, hw_components_dir_path, map_path, layer_shape_path)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m stats, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_timeloop_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhw_arch_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhw_components_dir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_shape_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper_config_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# debug\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/lab3/loaders.py:202\u001b[0m, in \u001b[0;36mrun_timeloop_mapper\u001b[0;34m(*paths)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_timeloop_mapper\u001b[39m(\u001b[38;5;241m*\u001b[39mpaths):\n\u001b[0;32m--> 202\u001b[0m     yaml_str \u001b[38;5;241m=\u001b[39m dump_str(\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    203\u001b[0m     mapper \u001b[38;5;241m=\u001b[39m MapperApp(yaml_str, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    204\u001b[0m     result \u001b[38;5;241m=\u001b[39m mapper\u001b[38;5;241m.\u001b[39mrun_subprocess()\n",
      "File \u001b[0;32m~/lab3/loaders.py:124\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(*paths)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 total \u001b[38;5;241m=\u001b[39m _collect_yaml(f\u001b[38;5;241m.\u001b[39mread(), total)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    125\u001b[0m             total \u001b[38;5;241m=\u001b[39m _collect_yaml(f\u001b[38;5;241m.\u001b[39mread(), total)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total\n",
      "File \u001b[0;32m/usr/lib/python3.8/pathlib.py:1222\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[0;32m-> 1222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m               \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/pathlib.py:1078\u001b[0m, in \u001b[0;36mPath._opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'designs/system_manual/arch/system_arch_1x16.yaml'"
     ]
    }
   ],
   "source": [
    "# Debug Conv3D\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv3d(20, 20, (3, 1, 1), stride=(1, 1, 1))\n",
    "# )\n",
    "# x = torch.rand(8, 20, 220, 5, 5)\n",
    "\n",
    "# Debug Conv2D\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(3, 5, (2, 2), stride=(1, 1))\n",
    "# )\n",
    "# x = torch.rand(1, 3, 5, 5)\n",
    "\n",
    "# Debug linear\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(30, 10)\n",
    "# )\n",
    "# x = torch.rand(8, 30)\n",
    "\n",
    "# Hamida\n",
    "model = HamidaEtAl(input_channels=220, n_classes=16)\n",
    "x = torch.rand(8, 1, 220, 5, 5)\n",
    "\n",
    "total, layerwise = get_energy(model, x, verbose=False)\n",
    "print(\"total energy:\", total)\n",
    "print(\"layerwise energy:\", layerwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
